{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "390a9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from preprocess_tweet import PreprocessTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "48895695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text data\n",
    "with open('twitter.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7987b7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3335477"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "53fb0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way \n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "046f09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    " def Preprocessing(data):\n",
    "    \"\"\"\n",
    "    Preprocess the text corpus\n",
    "    split the data by line breaks '\\n'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        corpus in raw format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list_sentences :  list\n",
    "        preprocessed list of list of tokens for each sentence.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split by linebreaks\n",
    "    sentences = data.split(\"\\n\")\n",
    "\n",
    "    # strip whitespaces\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    #remove empty sentence\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "    #tokenize, # and punctuation\n",
    "    list_sentences = [s.lower().split(\" \") for s in sentences]\n",
    "    \n",
    "    \n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "35ac2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = Preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "85006ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data:38368\n",
      "size of test data:9593\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_sentences)\n",
    "\n",
    "#80/20 train and test\n",
    "lt = int(len(tokenized_sentences)*0.8)\n",
    "train_data = tokenized_sentences[:lt]\n",
    "test_data = tokenized_sentences[lt:]\n",
    "\n",
    "print(f'size of train data:{len(train_data)}\\nsize of test data:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "a809db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "# get the count of all the words in the corpus\n",
    "\n",
    "flat_list = [el for sentence in tokenized_sentences for el in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "1ffb9100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 18863),\n",
       " ('to', 15647),\n",
       " ('i', 14263),\n",
       " ('a', 12254),\n",
       " ('you', 9732),\n",
       " ('and', 8660),\n",
       " ('for', 7716),\n",
       " ('in', 7469),\n",
       " ('is', 7268),\n",
       " ('of', 7221)]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = Counter(flat_list)\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f2374295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18863"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.get('the')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b329812",
   "metadata": {},
   "source": [
    "#### Handle OoV(Out of Vocabulary) word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf8f9b",
   "metadata": {},
   "source": [
    "1. create a closed vocabulary\n",
    "2. words that does not appear more frequently, assign then an unknows token <oov>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b9a04e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_closed_vocab(tokens, freq_threshold):\n",
    "    \"\"\"\n",
    "    create a closed vocabulary for a given text corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: list\n",
    "        list of sentence tokens\n",
    "    freq_threshold: int\n",
    "        word frequemcy threshold\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    closed_vocab : list\n",
    "        list of tokens having frequency greater than the threshold\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    closed_vocab = []\n",
    "    for sentence_t in tokens:\n",
    "        for word in sentence_t:\n",
    "            if word_count.get(word) > freq_threshold:\n",
    "                closed_vocab.append(word)\n",
    "                \n",
    "    return set(closed_vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578db757",
   "metadata": {},
   "source": [
    "**Method to create a closed form vocab:**\\\n",
    "    1. get the word count for each token for th given corpus.\\\n",
    "    2. create a closed form vocab using training set.\\\n",
    "    3. replace the unseen word in training set (from closed from vocab) with <oov> token.\\\n",
    "    4. relace the unseen word of test set with <oov> token as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ebcb5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get closed vocab using train set\n",
    "closed_vocab =  create_closed_vocab(train_data, freq_threshold=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5557e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14905"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "676220a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessTrainTest(data, vocab):\n",
    "    \"\"\"\n",
    "    Handles unknown/missing word in training corpus with <oov> token\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    data: list \n",
    "        list of list of tokens\n",
    "    vocab: set\n",
    "        set containing unique words in traiing data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    replaced_sentence: list\n",
    "        preprocessed list of list of tokens\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    #replace the token not in vocab with <oov>\n",
    "    replaced_sentence = []\n",
    "    for sentence in data:\n",
    "        temp_sentence = []\n",
    "        for token in sentence:\n",
    "            if token not in vocab:\n",
    "                temp_sentence.append('<oov>')\n",
    "            else:\n",
    "                temp_sentence.append(token)\n",
    "        replaced_sentence.append(temp_sentence)\n",
    "    \n",
    "    return replaced_sentence\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "371c906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 1.94 ms, total: 119 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess train data\n",
    "processed_train_data = PreprocessTrainTest(train_data, closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bf038300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train_data[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3ae68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "240c1bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.7 ms, sys: 1.98 ms, total: 46.7 ms\n",
      "Wall time: 45.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preproces test data\n",
    "processed_test_data = PreprocessTrainTest(test_data, closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5f9bf063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanks', 'for', 'the', 'follow', 'and', 'mention', '!']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad47901",
   "metadata": {},
   "source": [
    "# N-gram based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b5c10d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram(data, n=2, start_token= '<s>', end_token='<e>'):\n",
    "    \n",
    "    n_gram_dict = {}\n",
    "    for sentence in data:\n",
    "        # append n start token in the begining of each tokens\n",
    "        tokens = sentence.copy()\n",
    "        if n==1:\n",
    "            tokens.insert(0, start_token)\n",
    "\n",
    "        for i in range(n-1):\n",
    "            tokens.insert(i, start_token)\n",
    "        tokens.append(end_token)\n",
    "        \n",
    "        #create n gram count dictionary\n",
    "        for i in range(len(tokens)):\n",
    "            key = tuple(tokens[i:i+n])\n",
    "            if len(key) == n:\n",
    "\n",
    "                if key in n_gram_dict:\n",
    "                    n_gram_dict[key]+=1\n",
    "\n",
    "                else:\n",
    "                    n_gram_dict[key]= 1\n",
    "        \n",
    "    return n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "fc74b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4a58bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bb48a4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', 'i'): 1,\n",
       " ('i', 'like'): 1,\n",
       " ('like', 'a'): 2,\n",
       " ('a', 'cat'): 2,\n",
       " ('cat', '<e>'): 2,\n",
       " ('<s>', 'this'): 1,\n",
       " ('this', 'dog'): 1,\n",
       " ('dog', 'is'): 1,\n",
       " ('is', 'like'): 1}"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_n_gram(sentences, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5075c281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>',): 2,\n",
       " ('i',): 1,\n",
       " ('like',): 2,\n",
       " ('a',): 2,\n",
       " ('cat',): 2,\n",
       " ('<e>',): 2,\n",
       " ('this',): 1,\n",
       " ('dog',): 1,\n",
       " ('is',): 1}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_n_gram(sentences, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "624d79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate the porobaility\n",
    "# use k smoothing to handle the missing N gram\n",
    "\n",
    "def calculate_probabilities(previous_n_gram, vocab, prev_n_gram_count, n_gram_count, end_token ='<e>', oov_token='<oov>', k=1 ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    previous_n_gram: tuple \n",
    "        sequenc of previous words of length n\n",
    "    vocab: set\n",
    "        set of unique word in training corpus\n",
    "    prev_n_gram_count : dict\n",
    "        dictionary for prev n-gram count\n",
    "    n_gram_count : dict\n",
    "        dictionary for n+1 gram count\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    dictionary of joint probability of each word and previous n-gram word\n",
    "    \n",
    "    \"\"\"\n",
    "    # since start token ca not be the end of a n-gram\n",
    "    #we didn't include start token in vocab\n",
    "    vocab_new = vocab + [oov_token, end_token]\n",
    "    probabilities = {}\n",
    "    for word in vocab_new:\n",
    "        joint_words = previous_n_gram +(word,)\n",
    "\n",
    "        count = n_gram_count[joint_words] if joint_words in n_gram_count else 0\n",
    "        prev_count = prev_n_gram_count[previous_n_gram] if previous_n_gram in prev_n_gram_count else 0\n",
    "      \n",
    "        #apply k smoothing\n",
    "        prob = (count + k)/(prev_count + len(vocab)*k)\n",
    "        probabilities[word] = prob\n",
    "    \n",
    "    return probabilities\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "31403c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 0.1111111111111111,\n",
       " 'is': 0.1111111111111111,\n",
       " 'i': 0.1111111111111111,\n",
       " 'a': 0.1111111111111111,\n",
       " 'cat': 0.3333333333333333,\n",
       " 'this': 0.1111111111111111,\n",
       " 'like': 0.1111111111111111,\n",
       " '<oov>': 0.1111111111111111,\n",
       " '<e>': 0.1111111111111111}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_n_gram_count = create_n_gram(sentences, 1)\n",
    "n_gram_count = create_n_gram(sentences, 2)\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "# unique_words\n",
    "calculate_probabilities(('a',), unique_words.copy(),  prev_n_gram_count, n_gram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "05298abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'is', 'i', 'a', 'cat', 'this', 'like']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "09297278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot them using matrix\n",
    "def show_as_matrix(sentence, \n",
    "                   vocab, \n",
    "                   sentences,\n",
    "                   start_token='<s>', \n",
    "                   end_token ='<e>', \n",
    "                   oov_token='<oov>',\n",
    "                   n=2, \n",
    "                   k=1):\n",
    "    \"\"\"\n",
    "    Prints the count matrix and probability matrix as dataframe for better visualization.\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    n_gram = []\n",
    "    sentence = [start_token]*(n-1) + sentence\n",
    "\n",
    "     \n",
    "    vocab_new = list(set(vocab + [oov_token, end_token]))\n",
    "    \n",
    "    prev_n_gram_count = create_n_gram(sentences, n-1)\n",
    "    n_gram = list(set(prev_n_gram_count.keys())) \n",
    "    n_gram_count = create_n_gram(sentences, n)\n",
    "    \n",
    "    count_matrix = np.zeros((len(n_gram), len(vocab_new)))\n",
    "    prob_matrix = np.zeros((len(n_gram), len(vocab_new)))\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(len(n_gram)):\n",
    "        for j in range(len(vocab_new)):\n",
    "            n_token = n_gram[i] + (vocab_new[j],)\n",
    "            count_n_token = n_gram_count[n_token] if n_token in n_gram_count else 0\n",
    "            count_matrix[i,j] = count_n_token\n",
    "            prob_matrix[i,j] = calculate_probabilities(n_gram[i], vocab_new,prev_n_gram_count, n_gram_count)[vocab_new[j]]\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_gram, columns=vocab_new)\n",
    "    prob_matrix = pd.DataFrame(prob_matrix, index=n_gram, columns=vocab_new)\n",
    "    \n",
    "    return count_matrix, prob_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3434a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "19f69d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, pm = show_as_matrix(unique_words, unique_words,sentences, n=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "c197918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;oov&gt;</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat, &lt;e&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dog   is  <e>    i    a  cat  this  <oov>  like\n",
       "(<s>, i)     0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0   1.0\n",
       "(<s>, this)  1.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0   0.0\n",
       "(dog, is)    0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0   1.0\n",
       "(i, like)    0.0  0.0  0.0  0.0  1.0  0.0   0.0    0.0   0.0\n",
       "(is, like)   0.0  0.0  0.0  0.0  1.0  0.0   0.0    0.0   0.0\n",
       "(this, dog)  0.0  1.0  0.0  0.0  0.0  0.0   0.0    0.0   0.0\n",
       "(cat, <e>)   0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0   0.0\n",
       "(a, cat)     0.0  0.0  2.0  0.0  0.0  0.0   0.0    0.0   0.0\n",
       "(like, a)    0.0  0.0  0.0  0.0  0.0  2.0   0.0    0.0   0.0"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "10d90eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;oov&gt;</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat, &lt;e&gt;)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dog        is       <e>         i         a       cat  \\\n",
       "(<s>, i)     0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, this)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(dog, is)    0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(i, like)    0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(is, like)   0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(this, dog)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(cat, <e>)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(a, cat)     0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
       "(like, a)    0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "\n",
       "                 this     <oov>      like  \n",
       "(<s>, i)     0.100000  0.100000  0.200000  \n",
       "(<s>, this)  0.100000  0.100000  0.100000  \n",
       "(dog, is)    0.100000  0.100000  0.200000  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(this, dog)  0.100000  0.100000  0.100000  \n",
       "(cat, <e>)   0.090909  0.090909  0.090909  \n",
       "(a, cat)     0.090909  0.090909  0.090909  \n",
       "(like, a)    0.090909  0.090909  0.090909  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d63aedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the probability matrix and count matrix, Now we are ready to develop a autocomplete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d1b01b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "35e114c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_of_sentence(sentences, sentence, vocab, n = 2, start_token ='<s>', end_token ='<e>'):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    sentences: list of list of sentence tokens\n",
    "        list of words of sentence\n",
    "    sentence :  list \n",
    "        list of toknes whose pp is to be calculated\n",
    "    vocab :  list \n",
    "        list of unique words in the corpus\n",
    "    n : int\n",
    "        expected n gram\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pp : float\n",
    "        perplexity score for a given sentence\n",
    "        \n",
    "    \"\"\"\n",
    "#     sentence = sentences[1]\n",
    "    prev_n_gram_count = create_n_gram(sentences, n-1)\n",
    "    n_gram_count = create_n_gram(sentences, n)\n",
    "    \n",
    "    \n",
    "    sentence = [start_token]*(n-1) + sentence +[end_token]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "    \n",
    "    pp_prob = 1\n",
    "    for i in range(n-1, N):\n",
    "        n_gram = sentence[i-n+1:i]\n",
    "        \n",
    "        word = sentence[i]\n",
    "        \n",
    "        #include start and end token in calculating prob\n",
    "        # count the start and end token in sequence lenght also\n",
    "        prob = calculate_probabilities(n_gram, vocab, prev_n_gram_count, n_gram_count)[word]\n",
    "        pp_prob+=math.log((1/prob))\n",
    "    \n",
    "    \n",
    "    pp = pp_prob**(float(1/N))\n",
    "    return pp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ea059393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4492589235968028"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_of_sentence(sentences, test_sentence, unique_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a00bde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nextword(sentences, sentence, vocab, n = 2, start_token ='<s>', end_token ='<e>', begins_with='d', suggestion_freq=2):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    sentences: list of list of sentence tokens\n",
    "        list of words of sentence\n",
    "    sentence :  list \n",
    "        list of toknes whose pp is to be calculated\n",
    "    vocab :  list \n",
    "        list of unique words in the corpus\n",
    "    n : int\n",
    "        expected n gram\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a list of predicted k words where k is suggestion_freq\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #get previous n-1 word\n",
    "    sentence = tuple(sentence)\n",
    "    prev_word = sentence[-(n-1):]\n",
    "    \n",
    "    new_vocab = list(set(vocab + [end_token]))\n",
    "    prev_n_gram_count = create_n_gram(sentences, n-1)\n",
    "    n_gram_count = create_n_gram(sentences, n)\n",
    "    \n",
    "    \n",
    "    # get the probability for prev word and next candidate word\n",
    "    prob = calculate_probabilities(prev_word, new_vocab, prev_n_gram_count, n_gram_count)\n",
    "    prob =  {k: v for k, v in sorted(prob.items(), reverse=True,  key=lambda item: item[1])}\n",
    "    \n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for k, v in prob.items():\n",
    "        if len(begins_with) > 0:\n",
    "            if k.startswith(begins_with):\n",
    "                res.append({k:v})\n",
    "        else:\n",
    "            res.append({k:v})\n",
    "                \n",
    "                \n",
    "    return res[:suggestion_freq]\n",
    "    print(res[:suggestion_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7c19bb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'a']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "fa108438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dog': 0.1}]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nextword(sentences, sentences[0][:-1],unique_words, begins_with='d' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ece27076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocompleteSentence(sentences, sentence, vocab, n = 2, start_token ='<s>', end_token ='<e>'):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    sentences: list of list of sentence tokens\n",
    "        list of words of sentence\n",
    "    sentence :  list \n",
    "        list of toknes whose pp is to be calculated\n",
    "    vocab :  list \n",
    "        list of unique words in the corpus\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        complete sentence based on n-gram probability\n",
    "    \"\"\"\n",
    "    sentence = sentence.copy()\n",
    "    curr_word = start_token\n",
    "    while curr_word != end_token:\n",
    "        prev_word = list(sentence[-(n-1):])\n",
    "\n",
    "        curr_word = list(predict_nextword(sentences, prev_word, vocab, n, begins_with='', suggestion_freq=1)[0])[0]      \n",
    "\n",
    "        sentence.append(curr_word)\n",
    "        \n",
    "    return \" \".join(sentence[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "faf39fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'dog', 'is', 'like']"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentences[1][:-2]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "870783b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2914802650430801"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for n in range(2, 4):\n",
    "#     print(n)\n",
    "pred = autocompleteSentence(sentences, sentence, unique_words, n=3 )\n",
    "perplexity_of_sentence(sentences, pred.split(\" \"), unique_words, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "aca38fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 2-gram, PP: 1.3328333831401098\n",
      "for 3-gram, PP: 1.2914802650430801\n",
      "for 4-gram, PP: 1.2624417182398258\n",
      "for 5-gram, PP: 1.2390810140561817\n"
     ]
    }
   ],
   "source": [
    "# check the perplexity score for different n gram predicted sentence\n",
    "for n in range(2, 6):\n",
    "    \n",
    "    pred = autocompleteSentence(sentences, sentence, unique_words, n=n )\n",
    "    score = perplexity_of_sentence(sentences, pred.split(\" \"), unique_words, n=n)\n",
    "    \n",
    "    print(f\"for {n}-gram, PP: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fd609",
   "metadata": {},
   "source": [
    "## Now let's test on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "49906b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<oov> my mom just made my day (: ! she said that shes getting shirts and braclets for me to <oov>'"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = processed_test_data[7][:-3]\n",
    "autocompleteSentence(processed_train_data, sentence, list(closed_vocab), n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "41729cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<oov>',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'just',\n",
       " 'made',\n",
       " 'my',\n",
       " 'day',\n",
       " '(:',\n",
       " '!',\n",
       " 'she',\n",
       " 'said',\n",
       " 'that',\n",
       " 'shes',\n",
       " 'getting',\n",
       " 'shirts',\n",
       " 'and',\n",
       " 'braclets',\n",
       " 'for',\n",
       " 'me',\n",
       " 'on',\n",
       " 'christmas',\n",
       " '<oov>']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c51095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
